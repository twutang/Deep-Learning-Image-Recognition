{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Introduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/week08_Deep_Reinforcement_Learning_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2gPhGQSZ1era",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Reinforcement Learning: An Introduction üëæ\n",
        "\n",
        "In this tutorial we enter in the world of Deep Reinforcement Learning (DRL). In particular, we will first familiarize ourselves with some basic concepts of Reinforcement Learning (RL), then we will implement a classical tabular Q-learning method for the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) problem and finally, implement a Deep Q-learning approach for the [CartPole](https://gym.openai.com/envs/CartPole-v1/) problem.\n",
        "\n",
        "\n",
        "> \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://media2.giphy.com/media/46ib09ZL1SdWuREnj3/giphy.gif?cid=3640f6095c6e92762f3446634d90bc65) ![alt text](https://media0.giphy.com/media/d9QiBcfzg64Io/200w.webp?cid=3640f6095c6e93e92f30655873731752)![alt text](https://i.gifer.com/GpAY.gif)\n",
        "\n",
        "The gifs above, show the results obtained by [deepmind ](https://arxiv.org/pdf/1312.5602v1.pdf). They successfully trained an RL agent using deep Q-learning  to play classical Atari arcade games. Cool ain't it? Let's see how they did it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Reinforcement Learning üìë:\n",
        "RL is an area of machine learning that addresses how an agent can learn to behave on an specific environment in order to maximize a predefined goal.  Just like humans learn to walk, this ML field learns by trial and error, as an agent learns by interacting with the environment and getting feedback regarding the performed actions.\n",
        "\n",
        "![alt text](https://keon.io/images/deep-q-learning/rl.png)\n",
        "\n",
        "The above picture ([source](https://keon.io/deep-q-learning/)) gathers all the typical steps involved in any RL problem. \n",
        "\n",
        "1.   At time $t$ the agent observes the environment state denoted by $S_t$.\n",
        "2.   Based on this observation, chooses an action $A_t$.\n",
        "3.  After performing the action $A_t$ gets a reward $R_t$ from the environment.\n",
        "4. The agent observes a new state $S_{t+1}$.\n",
        "\n",
        "The goal of the agent is to maximize its cumulative reward, called return. Reinforcement learning methods consist of different approaches so the agent can learn behaviors to maximize its goal.\n",
        "\n",
        "Following we list the main concepts that you should familiarise yourselfves with:\n",
        "\n",
        "\n",
        "*   **State Space ($\\mathcal{S}$):** Refers to the set of possible states where an agent might find himself while interacting with the environment.  \n",
        "*   **Action Space ($\\mathcal{A}$):** Refers to the set of all valid actions in a given environment. Some environments, like Atari, have discrete action spaces, where only a finite number of moves are available to the agent (left, right, up and down). Other environments, like the control of a robotic arm, have continuous action spaces, where actions are real-valued vectors, e.g., move the robotic arm x= 3.51 cm, y = 1cm and z=0.1cm.\n",
        "* **Policy ($\\pi$):** A policy is a rule used by an agent to decide, given a state, what actions to take.  A policy can be deterministic, i.e., given a particular state the agent will always choose the same action to perform in this state, or stochastic, where for a particular state, an agent will choose from a set of actions with some probability distribution.\n",
        "\n",
        "*   **Return:**  The goal of the agent is to maximize the overall obtained discounted reward also known as return:\n",
        "\n",
        "$$G_t=\\sum ^{\\infty} _{k=0} \\gamma ^t R_{t+k+1}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Where $0\\leq\\gamma\\leq 1$ is the discounted factor, and regulates how far into the future the agent is \"looking\". If $\\gamma=0$, the agent is only concerned with the immediate reward, while if $ \\gamma=1$ the agent is concerned with the overall experience reward. Why would we ever want a discount factor, though? Don‚Äôt we just want to get all rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards may not converge to a finite value and is hard to deal with in equations.\n",
        "\n",
        "\n",
        " * **Value functions:** Refers to the value of a state $V(s)$, or state-action pair $Q(s,a)$. By value, we mean the expected return if you start in that state ($S_t=s$) or state-action $(S_t=s,A_t=a)$ pair, and then act according to your policy forever after. Mathematically, we can define state value function and state-action pair as follows:\n",
        "$ V\\left(S\\right) \\doteq \\mathop{\\mathbb{E_\\pi}} \\left[\\sum ^{\\infty} _{k=0} \\gamma ^t R_{t+k+1} \\mid S_t=s\\right].$ and $Q\\left(s,a\\right) \\doteq \\mathop{\\mathbb{E_\\pi}}\\left[\\sum ^{\\infty} _{k=0} \\gamma ^t R_{t+k+1}\\mid S_t=s,A_t=a\\right]$.\n"
      ]
    },
    {
      "metadata": {
        "id": "wmpPzxlbwvxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q-Learning üòç\n",
        "This family of RL methods try to learn an approximator of the action-value functions $Q(s,a)$  based on the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation), such that the update using a classical [gradient descent ](https://en.wikipedia.org/wiki/Gradient_descent) formulation is given by:\n",
        "$$Q\\left(s,a\\right)=Q\\left(s,a\\right)+ \\alpha \\left(r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)-Q\\left(s,a\\right)\\right).$$\n",
        "Where $\\alpha$ is the step size. \n",
        " Q-Learning updates the estimated reward at each time step and  uses the old estimate $ \\max _{a}Q\\left(s_{t+1},a\\right)$ to update the new ones. In a more algorithmic way, the Q-Learning process is the following:\n",
        "\n",
        "\n",
        "1.   Initialize Q-values at random $Q\\left(s,a\\right)$.\n",
        "2. Forever or until learning is stopped do:\n",
        "> 1.  Observe state $s$.\n",
        "> 2.   Take action $a$ according to your policy, e.g., $\\epsilon-greedy$.\n",
        "> 3.   Observe reward $r$ and new state $s_{t+1}$.\n",
        "> 4. Based on your actual estimates, compute $\\max _{a}Q\\left(s_{t+1},a\\right)$.\n",
        "> 5. Update your current estimate for  $Q\\left(s,a\\right)$:\n",
        "$$Q\\left(s,a\\right)=Q\\left(s,a\\right)+ \\alpha \\left(r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)-Q\\left(s,a\\right)\\right).$$\n",
        "\n",
        "Okay, now that we are familiar with Q-Learning lets jump to a real implementation of it. üôå\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p_9McSwZh5sT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# üïπÔ∏è Tabular Q-Learning with Frozen Lake  üïπÔ∏è\n",
        "‚ùÑÔ∏è‚ùÑÔ∏è Brace yourselves, winter is coming! ‚ùÑÔ∏è‚ùÑÔ∏è In this section we will teach an agent how to play  the [Frozen lake](https://gym.openai.com/envs/FrozenLake-v0/) game using  a classical tabular Q-learning . \n",
        "![alt text](https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/1ee37cfc3130057f828f19b3cee6066d41c1eeb4/Q%20learning/FrozenLake/frozenlake.png)\n",
        "\n",
        "Winter has arrived and you and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
        "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery (!!), so you won't always move in the direction you intend (stochastic environment), i.e., there is a probability $p$ that you move in the direcction selected and a probability $(1-p)$ that given the slippery ice, you move to a random position near positon. \n",
        "\n",
        "The lake is represented by a 4x4 grid and the location where the frisbee has landed (G) as well as the holes (H) is always the same for every new game. The game is restarted every time you have successfully recovered the frisbee or you have fallen into the cold waters. A reward of +1 is given every time you recover the frisbee and 0 other way.\n"
      ]
    },
    {
      "metadata": {
        "id": "nNrutC72jZRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 0: Import the needed libraries:**\n",
        "\n",
        "We will be using 3 libraries:\n",
        "\n",
        "* Numpy for our Qtable.\n",
        "* OpenAI Gym for our FrozenLake Environment\n",
        "* Random to generate random numbers\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Q0OxrnpgjyFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-Nx8PYnj4I5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Environment creation:**\n",
        "\n",
        "OpenAi is  a library composed of many environments that we can use to train our agents, in our case we choose to use the Frozen Lake."
      ]
    },
    {
      "metadata": {
        "id": "DWgurerLkNLe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeaPjILgkS7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q-table **\n",
        "\n",
        " Now, we'll create our Q-table. The goal of the Q-table is to store the estimates $Q\\left(s,a\\right)$ and retrieve them when necessary. In this game the states are represented by each of the 16 grid positions being 0 the starting position and 16 the goal position and the actions are 4: left, right, up and down. Our Q-table will have then $16 \\times 4$ positions, where the value of the first column of the first row represents the expected return of being in position 0 and taking left.\n",
        " \n",
        "The number of rows (states) and columns (actions) the table will have can also be obtained using the values action_size and the state_size from the OpenAI Gym library: *env.action_space.n* and* env.observation_space.n*.\n",
        " \n",
        "We initialize the table to 0.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eintO6cYk5qN",
        "colab_type": "code",
        "outputId": "e39b4db8-0981-4a59-eb15-c8ec3c7830cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G06GlU14k_JG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters** ‚öôÔ∏è\n",
        "\n",
        "Following, we specify the hyperparameters:\n"
      ]
    },
    {
      "metadata": {
        "id": "SYWdb_rHlFzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_episodes = 25000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate (alpha in the previous formulation)\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NVb-8n8Jlkhs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At first, we don't know how to interact with the environment (Q-table values set to 0), so we start exploring it by taking random action with probability $\\epsilon=1$, capturing the rewards obtained and updating the Q-values of the table accordingly. As time passes by, we start knowing more and more the environment, so we reduce (decay_rate) the probability of taking a random action and we start exploiting our knowledge, we choose the action that leads us to the highest reward, i.e., the one with the highest Q-value."
      ]
    },
    {
      "metadata": {
        "id": "rXq90uCMllKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7MJwgHeNr5ir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVu4B3F2lLl8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q-Learning** üß†\n",
        "\n",
        "Now we implement the Q learning algorithm: \n",
        "> 1.  Observe state $s$.\n",
        "> 2.   Choose a random value $v$ between 0 and 1.\n",
        "> 3. If $v<\\epsilon$, we choose a random action, otherwise we select the action with maximum $Q(s,a)$.\n",
        "> 3.   Observe reward $r$ and new state $s_{t+1}$.\n",
        "> 4. Based on your previous estimates, compute $\\max _{a}Q\\left(s_{t+1},a\\right)$.\n",
        "> 5. Update your current estimates for  $Q\\left(s,a\\right)$:\n",
        "$$Q\\left(s,a\\right)=Q\\left(s,a\\right)+ \\alpha \\left(r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)-Q\\left(s,a\\right)\\right).$$\n"
      ]
    },
    {
      "metadata": {
        "id": "eYbqMpg-liRd",
        "colab_type": "code",
        "outputId": "abb44a62-49da-4ab4-ded6-be67199c00b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.48484\n",
            "[[1.03140688e-01 6.70738096e-02 6.76069042e-02 4.30753298e-02]\n",
            " [1.55889295e-02 1.46555410e-02 2.05909775e-03 5.88854107e-02]\n",
            " [1.16114970e-02 7.97313629e-03 3.64769133e-03 2.34241640e-02]\n",
            " [1.47526647e-03 3.61727402e-05 1.40607345e-02 1.49426609e-02]\n",
            " [1.16119970e-01 1.28518523e-02 1.54142983e-02 5.70822423e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.46196328e-04 3.00788130e-06 2.19039442e-05 9.72552105e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.97830594e-03 8.44007687e-02 1.08405497e-03 1.18890015e-01]\n",
            " [3.02797484e-02 2.64612449e-01 4.41389111e-03 1.55672954e-02]\n",
            " [1.20271253e-01 3.01498100e-03 9.66098718e-03 2.36024386e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.52463980e-03 1.24162585e-01 7.43106566e-01 2.62309375e-03]\n",
            " [2.61594610e-01 9.78972017e-01 2.11259429e-01 9.88536822e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvcHhVx5lrWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Use our Q-table to play FrozenLake ! **üëæ\n",
        "\n",
        "  After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"!\n",
        "  \n",
        "  By running this cell, you can see our agent playing FrozenLake:"
      ]
    },
    {
      "metadata": {
        "id": "Hz65tCGSlzSc",
        "colab_type": "code",
        "outputId": "a1d259cb-4312-467b-8d20-26b6d71700e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4428
        }
      },
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "\n",
        "state = env.reset()\n",
        "step = 0\n",
        "done = False\n",
        "print(\"****************************************************\")\n",
        "print(\"STARTING \")\n",
        "env.render()\n",
        "for step in range(max_steps):\n",
        "\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    # We print the current step.\n",
        "    print(\"Number of steps\", step)\n",
        "    if done:\n",
        "      break\n",
        "    state = new_state\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "****************************************************\n",
            "STARTING \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 1\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 2\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 3\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 4\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 5\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Number of steps 6\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 7\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "Number of steps 8\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 9\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 10\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 11\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 12\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 13\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 14\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 15\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 16\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 17\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 18\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 19\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 20\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 21\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 22\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 23\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 24\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 25\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 26\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 27\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 28\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 29\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 30\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 31\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 32\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "Number of steps 33\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 34\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Number of steps 35\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Number of steps 36\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Number of steps 37\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Number of steps 38\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Number of steps 39\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iSroWmlroYiC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs see how many times our agent finds the frisbee üéâüéâüéâ\n",
        "\n",
        "To this end we will print the last step of the game."
      ]
    },
    {
      "metadata": {
        "id": "W3BUEqOzocSl",
        "colab_type": "code",
        "outputId": "578af3f8-7f9e-45dc-fe3b-6c7710bd12b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "games=5\n",
        "state = env.reset()\n",
        "step = 0\n",
        "done = False\n",
        "print(\"****************************************************\")\n",
        "print(\"STARTING \")\n",
        "for game in range(games):\n",
        "  for step in range(max_steps):\n",
        "\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = np.argmax(qtable[state,:])\n",
        "\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      if done:\n",
        "        # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "        env.render()\n",
        "\n",
        "        # We print the number of step it took.\n",
        "        print(\"Number of steps\", step)\n",
        "        done= False\n",
        "        break\n",
        "      state = new_state\n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "STARTING \n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 92\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_d3L77JNpUvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# üïπÔ∏èCartPole üïπÔ∏è\n",
        "That wasn't so hard üòé! How about trying to balance a pole so it does not fall? In this section we will address the [CartPole](https://gym.openai.com/envs/CartPole-v1/) problem, let's get to it!\n",
        "\n",
        "![alt text](https://keon.io/images/deep-q-learning/animation.gif)\n",
        "\n",
        "\n",
        "\n",
        " As before we will use Q-learning to train our agent, so let's start by constructing our Q-table. We first need to find out the number of columns and rows of it. By checking the environment specifications of [OpenAi](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), we see that the actions are left and right, so we need two columns for the actions. On the other hand, the state information is given by:\n",
        "\n",
        "        Num\tObservation                 Min         Max\n",
        "        0\tCart Position             -4.8            4.8\n",
        "        1\tCart Velocity             -Inf            Inf\n",
        "        2\tPole Angle                 -24 deg        24 deg\n",
        "        3\tPole Velocity At Tip      -Inf            Inf\n",
        "      \n",
        "   The cart position goes from -4.8 to 4.8 with a resolution of 0.01, that means $\\frac{4.8 \\times 2}{0.01}=960$ possible carts positions, while the cart velocity goes from $-\\infty$ to $\\infty$!!üòµüòµ Ohhhhh boi... How we are going to construct a table with $\\infty$ rows[?!?!](https://www.youtube.com/watch?v=C3J1AO9z0tA&t=1m19s) \n",
        "   \n",
        " ![alt text](https://media1.giphy.com/media/HP0JqpBxLLGpO/giphy.gif?cid=3640f6095c71dd267138353159049ceb)\n",
        " \n",
        " \n",
        "Do not panic! That is when deep learning steps up and takes over the stage üéâüéâüéâ. As you have already seen the use of deep Neural Networks as general function approximators have been proven to work very well in wide range of areas, reinforcement learning is not an exception. In this case we will use the NNs as function approximation between the mapping of states to actions, so for every input state, we want the NNs to output an approximation of the $Q\\left(s,a\\right)$.\n",
        "\n",
        "![alt text](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1318%2F1*Gh5PS4R_A5drl5ebd_gNrg%402x.png&f=1)\n",
        "\n",
        "In this particular scenario, the input layer will have the same number of inputs as environment parameters, 4, and the output layer will have the same number of outputs as actions, in this case 2. \n",
        "\n",
        "**Reward:** A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical.\n"
      ]
    },
    {
      "metadata": {
        "id": "wHi3LRul2b6h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 0: Import the needed libraries**\n",
        "\n",
        "We start by importing the needed libraries:\n",
        "We will be using 3 libraries:\n",
        "* Keras: for our DNNs.\n",
        "* OpenAI Gym: for our CartPole Environment\n",
        "* Random: to generate random numbers.\n",
        "* Collections: Collection will be use to create a memory buffer to store the tuples $\\left(S_t, A_t, R_t,S_{t+1}\\right)$ of transactions. \n",
        "\n",
        "The idea behind the use of a memory buffer is that most optimization algorithms, including gradient descent, assume that the samples which the gradient is obtained are independent and identically distributed. Clearly in the defined environment is not the case, however, by sampling uniformly the memory buffer with a high number of samples the correlation between contiguous samples is broken and less likely to be correlated samples are used to update the networks weights, leading to a stable optimization of the action-parameter selection.\n"
      ]
    },
    {
      "metadata": {
        "id": "yO4Qbc942zPJ",
        "colab_type": "code",
        "outputId": "5b8fe875-400f-4acd-beeb-e19516a1cada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7KDyanK23SQH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Agent**\n",
        "\n",
        "Let's start by coding a general DQ-Learning agent. The state and action size are passed as parameters and we configure a replay buffer to have capacity to store 2000 experienced transitions."
      ]
    },
    {
      "metadata": {
        "id": "18b_7T9K-V_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xws00CC6Ae7K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we address the DDNs; we are going to use two fully connected layers of 24 neurons each and as an optimizer we select  adam optimizer."
      ]
    },
    {
      "metadata": {
        "id": "UUxh0JOWAtPT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    def _build_model(self):\n",
        "            # Neural Net for Deep-Q learning Model\n",
        "            model = Sequential()\n",
        "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "            model.add(Dense(24, activation='relu'))\n",
        "            model.add(Dense(self.action_size, activation='linear'))\n",
        "            model.compile(loss='mse',\n",
        "                          optimizer=Adam(lr=self.learning_rate))\n",
        "            return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yA4DQkqcBQ84",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now define the method to store the transictions into the memory buffer.\n",
        "The parameter done is a boolean returned true when the pole has fallen. "
      ]
    },
    {
      "metadata": {
        "id": "9qv7a_raBQGr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    def remember(self, state, action, reward, next_state, done):\n",
        "          self.memory.append((state, action, reward, next_state, done))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9X-HisYBc9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, we implement and $\\epsilon-greedy$ policy."
      ]
    },
    {
      "metadata": {
        "id": "AXJoxwi2Bmss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "     def act(self, state):\n",
        "            if np.random.rand() <= self.epsilon:\n",
        "                return random.randrange(self.action_size)\n",
        "            act_values = self.model.predict(state)\n",
        "            return np.argmax(act_values[0]) # returns action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9bmi6bk5Bv7E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then comes the implementation of the Q-Learning method:\n",
        "\n",
        "\n",
        "\n",
        "1.   We obtain the samples to train the DNN from the replay buffer.\n",
        "2.  We compute $target=r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)$, by doing a forward pass using next_state value.\n",
        "3. We do a forward pass through the network to obtain the $Q\\left(s,a\\right)$ for all the possible actions.\n",
        "4. In order to just update the paramater of the action taken, we copy target to the value of the $Q\\left(s,a\\right)$ of the actual $a$ taken.\n",
        "5. We update the parameters of the network using MSE as loss function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nYgDWxUmBu8n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    def replay(self, batch_size):\n",
        "            minibatch = random.sample(self.memory, batch_size)\n",
        "            for state, action, reward, next_state, done in minibatch:\n",
        "                target = reward\n",
        "                if not done:\n",
        "                    target = (reward + self.gamma *\n",
        "                              np.amax(self.model.predict(next_state)[0]))\n",
        "                target_f = self.model.predict(state)\n",
        "                target_f[0][action] = target\n",
        "                self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2E-OJlAFraT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define the operations to load and save the models."
      ]
    },
    {
      "metadata": {
        "id": "BeE1SshPFqrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M4J6L3xbF10Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code Lab might not recognize all the previous code parts defined under the same class, to this end we run everything this time altogether. "
      ]
    },
    {
      "metadata": {
        "id": "CFSDV2qAFYRk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):# We implement the epsilon-greedy policy\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0]) # returns action\n",
        "      \n",
        "    def exploit(self, state): # When we test the agent we dont want it to explore anymore, but to exploit what it has learnt\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0]) \n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t6YDZhTIDjiC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Main**\n",
        "\n",
        "Following we implement the training of the agent. (Warning: it might take a while)"
      ]
    },
    {
      "metadata": {
        "id": "MwA2WXrnDpzV",
        "colab_type": "code",
        "outputId": "fd887df0-84c3-4866-b08f-f37ee0a220d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4719
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    EPISODES = 500\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "episode: 0/500, score: 12, e: 1.0\n",
            "episode: 1/500, score: 13, e: 1.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "episode: 2/500, score: 21, e: 0.92\n",
            "episode: 3/500, score: 30, e: 0.79\n",
            "episode: 4/500, score: 17, e: 0.73\n",
            "episode: 5/500, score: 11, e: 0.69\n",
            "episode: 6/500, score: 22, e: 0.62\n",
            "episode: 7/500, score: 9, e: 0.59\n",
            "episode: 8/500, score: 9, e: 0.56\n",
            "episode: 9/500, score: 9, e: 0.54\n",
            "episode: 10/500, score: 12, e: 0.51\n",
            "episode: 11/500, score: 11, e: 0.48\n",
            "episode: 12/500, score: 18, e: 0.44\n",
            "episode: 13/500, score: 65, e: 0.32\n",
            "episode: 14/500, score: 50, e: 0.25\n",
            "episode: 15/500, score: 59, e: 0.18\n",
            "episode: 16/500, score: 47, e: 0.15\n",
            "episode: 17/500, score: 76, e: 0.099\n",
            "episode: 18/500, score: 64, e: 0.072\n",
            "episode: 19/500, score: 74, e: 0.05\n",
            "episode: 20/500, score: 108, e: 0.029\n",
            "episode: 21/500, score: 94, e: 0.018\n",
            "episode: 22/500, score: 125, e: 0.01\n",
            "episode: 23/500, score: 76, e: 0.01\n",
            "episode: 24/500, score: 62, e: 0.01\n",
            "episode: 25/500, score: 76, e: 0.01\n",
            "episode: 26/500, score: 59, e: 0.01\n",
            "episode: 27/500, score: 101, e: 0.01\n",
            "episode: 28/500, score: 140, e: 0.01\n",
            "episode: 29/500, score: 96, e: 0.01\n",
            "episode: 30/500, score: 318, e: 0.01\n",
            "episode: 31/500, score: 386, e: 0.01\n",
            "episode: 32/500, score: 318, e: 0.01\n",
            "episode: 33/500, score: 290, e: 0.01\n",
            "episode: 34/500, score: 8, e: 0.01\n",
            "episode: 35/500, score: 217, e: 0.01\n",
            "episode: 36/500, score: 217, e: 0.01\n",
            "episode: 37/500, score: 126, e: 0.01\n",
            "episode: 38/500, score: 109, e: 0.01\n",
            "episode: 39/500, score: 133, e: 0.01\n",
            "episode: 40/500, score: 135, e: 0.01\n",
            "episode: 41/500, score: 214, e: 0.01\n",
            "episode: 42/500, score: 260, e: 0.01\n",
            "episode: 43/500, score: 169, e: 0.01\n",
            "episode: 44/500, score: 45, e: 0.01\n",
            "episode: 45/500, score: 9, e: 0.01\n",
            "episode: 46/500, score: 41, e: 0.01\n",
            "episode: 47/500, score: 140, e: 0.01\n",
            "episode: 48/500, score: 185, e: 0.01\n",
            "episode: 49/500, score: 103, e: 0.01\n",
            "episode: 50/500, score: 499, e: 0.01\n",
            "episode: 51/500, score: 499, e: 0.01\n",
            "episode: 52/500, score: 499, e: 0.01\n",
            "episode: 53/500, score: 499, e: 0.01\n",
            "episode: 54/500, score: 367, e: 0.01\n",
            "episode: 55/500, score: 17, e: 0.01\n",
            "episode: 56/500, score: 12, e: 0.01\n",
            "episode: 57/500, score: 9, e: 0.01\n",
            "episode: 58/500, score: 10, e: 0.01\n",
            "episode: 59/500, score: 13, e: 0.01\n",
            "episode: 60/500, score: 151, e: 0.01\n",
            "episode: 61/500, score: 83, e: 0.01\n",
            "episode: 62/500, score: 79, e: 0.01\n",
            "episode: 63/500, score: 79, e: 0.01\n",
            "episode: 64/500, score: 98, e: 0.01\n",
            "episode: 65/500, score: 222, e: 0.01\n",
            "episode: 66/500, score: 221, e: 0.01\n",
            "episode: 67/500, score: 460, e: 0.01\n",
            "episode: 68/500, score: 305, e: 0.01\n",
            "episode: 69/500, score: 289, e: 0.01\n",
            "episode: 70/500, score: 153, e: 0.01\n",
            "episode: 71/500, score: 263, e: 0.01\n",
            "episode: 72/500, score: 250, e: 0.01\n",
            "episode: 73/500, score: 449, e: 0.01\n",
            "episode: 74/500, score: 209, e: 0.01\n",
            "episode: 75/500, score: 11, e: 0.01\n",
            "episode: 76/500, score: 8, e: 0.01\n",
            "episode: 77/500, score: 9, e: 0.01\n",
            "episode: 78/500, score: 127, e: 0.01\n",
            "episode: 79/500, score: 10, e: 0.01\n",
            "episode: 80/500, score: 266, e: 0.01\n",
            "episode: 81/500, score: 499, e: 0.01\n",
            "episode: 82/500, score: 499, e: 0.01\n",
            "episode: 83/500, score: 499, e: 0.01\n",
            "episode: 84/500, score: 451, e: 0.01\n",
            "episode: 85/500, score: 12, e: 0.01\n",
            "episode: 86/500, score: 12, e: 0.01\n",
            "episode: 87/500, score: 36, e: 0.01\n",
            "episode: 88/500, score: 39, e: 0.01\n",
            "episode: 89/500, score: 499, e: 0.01\n",
            "episode: 90/500, score: 218, e: 0.01\n",
            "episode: 91/500, score: 481, e: 0.01\n",
            "episode: 92/500, score: 499, e: 0.01\n",
            "episode: 93/500, score: 489, e: 0.01\n",
            "episode: 94/500, score: 132, e: 0.01\n",
            "episode: 95/500, score: 230, e: 0.01\n",
            "episode: 96/500, score: 107, e: 0.01\n",
            "episode: 97/500, score: 163, e: 0.01\n",
            "episode: 98/500, score: 56, e: 0.01\n",
            "episode: 99/500, score: 499, e: 0.01\n",
            "episode: 100/500, score: 499, e: 0.01\n",
            "episode: 101/500, score: 238, e: 0.01\n",
            "episode: 102/500, score: 499, e: 0.01\n",
            "episode: 103/500, score: 499, e: 0.01\n",
            "episode: 104/500, score: 155, e: 0.01\n",
            "episode: 105/500, score: 499, e: 0.01\n",
            "episode: 106/500, score: 499, e: 0.01\n",
            "episode: 107/500, score: 499, e: 0.01\n",
            "episode: 108/500, score: 499, e: 0.01\n",
            "episode: 109/500, score: 70, e: 0.01\n",
            "episode: 110/500, score: 32, e: 0.01\n",
            "episode: 111/500, score: 17, e: 0.01\n",
            "episode: 112/500, score: 7, e: 0.01\n",
            "episode: 113/500, score: 134, e: 0.01\n",
            "episode: 114/500, score: 150, e: 0.01\n",
            "episode: 115/500, score: 172, e: 0.01\n",
            "episode: 116/500, score: 209, e: 0.01\n",
            "episode: 117/500, score: 169, e: 0.01\n",
            "episode: 118/500, score: 203, e: 0.01\n",
            "episode: 119/500, score: 217, e: 0.01\n",
            "episode: 120/500, score: 159, e: 0.01\n",
            "episode: 121/500, score: 183, e: 0.01\n",
            "episode: 122/500, score: 154, e: 0.01\n",
            "episode: 123/500, score: 161, e: 0.01\n",
            "episode: 124/500, score: 179, e: 0.01\n",
            "episode: 125/500, score: 217, e: 0.01\n",
            "episode: 126/500, score: 55, e: 0.01\n",
            "episode: 127/500, score: 21, e: 0.01\n",
            "episode: 128/500, score: 14, e: 0.01\n",
            "episode: 129/500, score: 72, e: 0.01\n",
            "episode: 130/500, score: 11, e: 0.01\n",
            "episode: 131/500, score: 10, e: 0.01\n",
            "episode: 132/500, score: 13, e: 0.01\n",
            "episode: 133/500, score: 21, e: 0.01\n",
            "episode: 134/500, score: 120, e: 0.01\n",
            "episode: 135/500, score: 142, e: 0.01\n",
            "episode: 136/500, score: 186, e: 0.01\n",
            "episode: 137/500, score: 160, e: 0.01\n",
            "episode: 138/500, score: 211, e: 0.01\n",
            "episode: 139/500, score: 165, e: 0.01\n",
            "episode: 140/500, score: 166, e: 0.01\n",
            "episode: 141/500, score: 158, e: 0.01\n",
            "episode: 142/500, score: 166, e: 0.01\n",
            "episode: 143/500, score: 154, e: 0.01\n",
            "episode: 144/500, score: 160, e: 0.01\n",
            "episode: 145/500, score: 156, e: 0.01\n",
            "episode: 146/500, score: 147, e: 0.01\n",
            "episode: 147/500, score: 148, e: 0.01\n",
            "episode: 148/500, score: 169, e: 0.01\n",
            "episode: 149/500, score: 243, e: 0.01\n",
            "episode: 150/500, score: 198, e: 0.01\n",
            "episode: 151/500, score: 169, e: 0.01\n",
            "episode: 152/500, score: 160, e: 0.01\n",
            "episode: 153/500, score: 140, e: 0.01\n",
            "episode: 154/500, score: 151, e: 0.01\n",
            "episode: 155/500, score: 97, e: 0.01\n",
            "episode: 156/500, score: 175, e: 0.01\n",
            "episode: 157/500, score: 209, e: 0.01\n",
            "episode: 158/500, score: 185, e: 0.01\n",
            "episode: 159/500, score: 254, e: 0.01\n",
            "episode: 160/500, score: 147, e: 0.01\n",
            "episode: 161/500, score: 71, e: 0.01\n",
            "episode: 162/500, score: 10, e: 0.01\n",
            "episode: 163/500, score: 12, e: 0.01\n",
            "episode: 164/500, score: 43, e: 0.01\n",
            "episode: 165/500, score: 18, e: 0.01\n",
            "episode: 166/500, score: 242, e: 0.01\n",
            "episode: 167/500, score: 180, e: 0.01\n",
            "episode: 168/500, score: 250, e: 0.01\n",
            "episode: 169/500, score: 183, e: 0.01\n",
            "episode: 170/500, score: 188, e: 0.01\n",
            "episode: 171/500, score: 208, e: 0.01\n",
            "episode: 172/500, score: 193, e: 0.01\n",
            "episode: 173/500, score: 206, e: 0.01\n",
            "episode: 174/500, score: 184, e: 0.01\n",
            "episode: 175/500, score: 197, e: 0.01\n",
            "episode: 176/500, score: 186, e: 0.01\n",
            "episode: 177/500, score: 226, e: 0.01\n",
            "episode: 178/500, score: 48, e: 0.01\n",
            "episode: 179/500, score: 124, e: 0.01\n",
            "episode: 180/500, score: 131, e: 0.01\n",
            "episode: 181/500, score: 198, e: 0.01\n",
            "episode: 182/500, score: 72, e: 0.01\n",
            "episode: 183/500, score: 87, e: 0.01\n",
            "episode: 184/500, score: 98, e: 0.01\n",
            "episode: 185/500, score: 125, e: 0.01\n",
            "episode: 186/500, score: 125, e: 0.01\n",
            "episode: 187/500, score: 38, e: 0.01\n",
            "episode: 188/500, score: 114, e: 0.01\n",
            "episode: 189/500, score: 166, e: 0.01\n",
            "episode: 190/500, score: 145, e: 0.01\n",
            "episode: 191/500, score: 221, e: 0.01\n",
            "episode: 192/500, score: 185, e: 0.01\n",
            "episode: 193/500, score: 141, e: 0.01\n",
            "episode: 194/500, score: 134, e: 0.01\n",
            "episode: 195/500, score: 170, e: 0.01\n",
            "episode: 196/500, score: 156, e: 0.01\n",
            "episode: 197/500, score: 195, e: 0.01\n",
            "episode: 198/500, score: 152, e: 0.01\n",
            "episode: 199/500, score: 161, e: 0.01\n",
            "episode: 200/500, score: 63, e: 0.01\n",
            "episode: 201/500, score: 147, e: 0.01\n",
            "episode: 202/500, score: 164, e: 0.01\n",
            "episode: 203/500, score: 155, e: 0.01\n",
            "episode: 204/500, score: 158, e: 0.01\n",
            "episode: 205/500, score: 172, e: 0.01\n",
            "episode: 206/500, score: 181, e: 0.01\n",
            "episode: 207/500, score: 148, e: 0.01\n",
            "episode: 208/500, score: 189, e: 0.01\n",
            "episode: 209/500, score: 209, e: 0.01\n",
            "episode: 210/500, score: 215, e: 0.01\n",
            "episode: 211/500, score: 173, e: 0.01\n",
            "episode: 212/500, score: 139, e: 0.01\n",
            "episode: 213/500, score: 187, e: 0.01\n",
            "episode: 214/500, score: 184, e: 0.01\n",
            "episode: 215/500, score: 216, e: 0.01\n",
            "episode: 216/500, score: 120, e: 0.01\n",
            "episode: 217/500, score: 181, e: 0.01\n",
            "episode: 218/500, score: 190, e: 0.01\n",
            "episode: 219/500, score: 191, e: 0.01\n",
            "episode: 220/500, score: 25, e: 0.01\n",
            "episode: 221/500, score: 193, e: 0.01\n",
            "episode: 222/500, score: 115, e: 0.01\n",
            "episode: 223/500, score: 82, e: 0.01\n",
            "episode: 224/500, score: 307, e: 0.01\n",
            "episode: 225/500, score: 368, e: 0.01\n",
            "episode: 226/500, score: 271, e: 0.01\n",
            "episode: 227/500, score: 257, e: 0.01\n",
            "episode: 228/500, score: 100, e: 0.01\n",
            "episode: 229/500, score: 217, e: 0.01\n",
            "episode: 230/500, score: 253, e: 0.01\n",
            "episode: 231/500, score: 243, e: 0.01\n",
            "episode: 232/500, score: 244, e: 0.01\n",
            "episode: 233/500, score: 220, e: 0.01\n",
            "episode: 234/500, score: 188, e: 0.01\n",
            "episode: 235/500, score: 231, e: 0.01\n",
            "episode: 236/500, score: 247, e: 0.01\n",
            "episode: 237/500, score: 243, e: 0.01\n",
            "episode: 238/500, score: 237, e: 0.01\n",
            "episode: 239/500, score: 499, e: 0.01\n",
            "episode: 240/500, score: 11, e: 0.01\n",
            "episode: 241/500, score: 268, e: 0.01\n",
            "episode: 242/500, score: 243, e: 0.01\n",
            "episode: 243/500, score: 349, e: 0.01\n",
            "episode: 244/500, score: 233, e: 0.01\n",
            "episode: 245/500, score: 284, e: 0.01\n",
            "episode: 246/500, score: 282, e: 0.01\n",
            "episode: 247/500, score: 469, e: 0.01\n",
            "episode: 248/500, score: 473, e: 0.01\n",
            "episode: 249/500, score: 499, e: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W2TqM_HhGVM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now visualize how the agent is performing:"
      ]
    },
    {
      "metadata": {
        "id": "lGglc_BfYam-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMUl6WrhYtuj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WO20MXmfNs4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tKv6DNBDGY0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make('CartPole-v1'))\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "done = False\n",
        "state = env.reset()\n",
        "state = np.reshape(state, [1, state_size])\n",
        "for time in range(500):\n",
        "    screen = env.render()\n",
        "    action = agent.exploit(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "\n",
        "    if done:\n",
        "        print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "              .format(e, EPISODES, time, agent.epsilon))\n",
        "        break\n",
        "env.close()\n",
        "show_video()\n",
        "env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wg2wp6Duu7Ok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Don't stop me now üôà!!\n",
        "![alt text](https://media0.giphy.com/media/GIIC4jmmUlXZS/giphy.gif?cid=3640f6095c71dc3e396a754159b0c5af)\n",
        "\n",
        "If you are thirsty for more, why not trying new environments of OpenAI such as[ mountain car ](https://gym.openai.com/envs/MountainCarContinuous-v0/) using the same RL as Q-learning or, to reduce variance and increase performance implement a [double Q-Learning](https://arxiv.org/pdf/1509.06461.pdf)?\n",
        "\n",
        "If you really fancy DRL, have a look of the tutorials and code prepared by [OpenAI](https://spinningup.openai.com/en/latest/user/introduction.html)."
      ]
    }
  ]
}